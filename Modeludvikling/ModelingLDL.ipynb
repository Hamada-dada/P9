{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_excel(r\"C:\\Users\\kfq6\\Documents\\Data\\Sammedag_master_HbA1c_Features.xlsx\")\n",
    "#drop rows where target variable is missing\n",
    "df_model = df.dropna(subset=[\"HbA1c_next\"]).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2) Encode sex\n",
    "df_model[\"sex_male\"] = (df_model[\"sex\"] == \"M\").astype(int)\n",
    "\n",
    "# 3) One-hot diagnosis\n",
    "diag_dummies = pd.get_dummies(df_model[\"diagnosis\"], prefix=\"dx\")\n",
    "df_model = pd.concat([df_model, diag_dummies], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Optional missingness indicators\n",
    "for col in [\"LABmean__u_albumin_kreatinin_ratio_mg_g\",\n",
    "            \"HbA1c_slope_prev_year\", \"HbA1c_CV_prev_year\", \"HbA1c_MAC_prev_year\"]:\n",
    "    df_model[f\"has_{col}\"] = df_model[col].notna().astype(int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Define feature columns\n",
    "drop_cols = [\n",
    "    \"DW_EK_Borger\",\n",
    "    \"sex\",\n",
    "    \"diagnosis\",\n",
    "    \"anchor_date\",\n",
    "    \"HbA1c_next\",\n",
    "    \"year_next\",\n",
    "    \"anchor_date_next\",\n",
    "    \"delta_years_to_next\",\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in df_model.columns if c not in drop_cols]\n",
    "\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[\"HbA1c_next\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ID_COL = \"DW_EK_Borger\"\n",
    "unique_ids = df_model[ID_COL].unique()\n",
    "\n",
    "train_ids, test_ids = train_test_split(\n",
    "    unique_ids,\n",
    "    test_size=0.3,     # fx 20% test\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = df_model[df_model[ID_COL].isin(train_ids)].copy()\n",
    "test_df  = df_model[df_model[ID_COL].isin(test_ids)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MAE (per fold): [5.85104261 5.45364815 6.46075529 6.12729416 6.71551982]\n",
      "CV MAE (mean): 6.12165200782375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "target_col = \"HbA1c_next\"\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "groups  = train_df[ID_COL]\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=gkf,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    groups=groups\n",
    ")\n",
    "\n",
    "print(\"CV MAE (per fold):\", -cv_scores)\n",
    "print(\"CV MAE (mean):\", -cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST MAE:  5.89\n",
      "TEST RMSE: 8.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_test_pred)\n",
    "rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "print(f\"TEST MAE:  {mae:.2f}\")\n",
    "print(f\"TEST RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R^2: 0.459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"TEST R^2: {r2:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 (manual): 0.4591254195204342\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array(y_test)\n",
    "y_pred = np.array(y_test_pred)\n",
    "\n",
    "ss_res = np.sum((y_true - y_pred) ** 2)        # residual sum of squares\n",
    "ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # total sum of squares\n",
    "\n",
    "r2_manual = 1 - ss_res / ss_tot\n",
    "print(\"R^2 (manual):\", r2_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Best params: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 8, 'max_features': 0.7, 'max_depth': 10, 'bootstrap': True}\n",
      "Best CV R^2: 0.5272990331896266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "ID_COL = \"DW_EK_Borger\"\n",
    "\n",
    "# Groups = patients\n",
    "groups = train_df[ID_COL]\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Reasonable hyperparameter search space\n",
    "param_dist = {\n",
    "    \"n_estimators\":     [200, 300, 400, 600, 800],\n",
    "    \"max_depth\":        [None, 5, 10, 15, 20],\n",
    "    \"min_samples_split\":[2, 5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 8],\n",
    "    \"max_features\":     [\"sqrt\", \"log2\", 0.3, 0.5, 0.7],\n",
    "    \"bootstrap\":        [True, False],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=80,              # bump to 80 if you’re feeling masochistic\n",
    "    scoring=\"r2\",           # or \"neg_mean_absolute_error\" if you want to optimise MAE\n",
    "    cv=gkf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train, groups=groups)\n",
    "\n",
    "print(\"Best params:\", search.best_params_)\n",
    "print(\"Best CV R^2:\", search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best XGB params: {'subsample': 0.6, 'n_estimators': 400, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "Best XGB CV R^2: 0.5273896701232175\n",
      "XGB TEST R²:  0.483\n",
      "XGB TEST MAE: 5.76\n",
      "XGB TEST RMSE:7.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",  # faster, if available\n",
    ")\n",
    "\n",
    "param_dist_xgb = {\n",
    "    \"n_estimators\": [200, 400, 600, 800],\n",
    "    \"max_depth\":    [3, 4, 5, 6],\n",
    "    \"learning_rate\":[0.01, 0.03, 0.05, 0.1],\n",
    "    \"subsample\":    [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"gamma\":        [0, 0.1, 0.3],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "}\n",
    "\n",
    "search_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=40,\n",
    "    scoring=\"r2\",\n",
    "    cv=gkf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "search_xgb.fit(X_train, y_train, groups=groups)\n",
    "\n",
    "print(\"Best XGB params:\", search_xgb.best_params_)\n",
    "print(\"Best XGB CV R^2:\", search_xgb.best_score_)\n",
    "\n",
    "best_xgb = search_xgb.best_estimator_\n",
    "\n",
    "y_test_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "r2_test_xgb  = r2_score(y_test, y_test_pred_xgb)\n",
    "mae_test_xgb = mean_absolute_error(y_test, y_test_pred_xgb)\n",
    "rmse_test_xgb = mean_squared_error(y_test, y_test_pred_xgb, squared=False)\n",
    "\n",
    "print(f\"XGB TEST R²:  {r2_test_xgb:.3f}\")\n",
    "print(f\"XGB TEST MAE: {mae_test_xgb:.2f}\")\n",
    "print(f\"XGB TEST RMSE:{rmse_test_xgb:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (global mean) - MAE: 8.71, RMSE: 11.06\n",
      "Baseline (same as current) - MAE: 6.07, RMSE: 8.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Global mean baseline\n",
    "mean_baseline = y_train.mean()\n",
    "y_test_pred_mean = np.full_like(y_test, fill_value=mean_baseline, dtype=float)\n",
    "\n",
    "mae_mean = mean_absolute_error(y_test, y_test_pred_mean)\n",
    "rmse_mean = mean_squared_error(y_test, y_test_pred_mean, squared=False)\n",
    "\n",
    "print(f\"Baseline (global mean) - MAE: {mae_mean:.2f}, RMSE: {rmse_mean:.2f}\")\n",
    "\n",
    "# \"Predict current HbA1c\" baseline\n",
    "current_col = \"LABmean__hb_b_haemoglobin_a1c_ifcc_mmol_mol\"\n",
    "y_test_pred_same = test_df[current_col].values\n",
    "\n",
    "mae_same = mean_absolute_error(y_test, y_test_pred_same)\n",
    "rmse_same = mean_squared_error(y_test, y_test_pred_same, squared=False)\n",
    "\n",
    "print(f\"Baseline (same as current) - MAE: {mae_same:.2f}, RMSE: {rmse_same:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        feature  importance\n",
      "12  LABmean__hb_b_haemoglobin_a1c_ifcc_mmol_mol    0.198026\n",
      "0                                           age    0.038825\n",
      "21      LABmean__u_albumin_kreatinin_ratio_mg_g    0.033392\n",
      "29                           dx_Type 1-diabetes    0.032512\n",
      "30                           dx_Type 2-diabetes    0.032326\n",
      "11          LABmean__egfr_1_73m2_ckd_epi_ml_min    0.030595\n",
      "19                LABmean__p_triglycerid_mmol_l    0.030088\n",
      "25                          HbA1c_MAC_prev_year    0.030035\n",
      "32                    has_HbA1c_slope_prev_year    0.029116\n",
      "23                        HbA1c_slope_prev_year    0.028773\n",
      "24                           HbA1c_CV_prev_year    0.028691\n",
      "14             LABmean__p_kolesterol_hdl_mmol_l    0.028140\n",
      "5                                     comp_foot    0.027555\n",
      "1                                      comp_eye    0.026592\n",
      "17                  LABmean__p_kreatinin_umol_l    0.026049\n",
      "7                                      comp_uns    0.025823\n",
      "16                 LABmean__p_kolesterol_mmol_l    0.025741\n",
      "18                    LABmean__p_natrium_mmol_l    0.025021\n",
      "27                 dx_Andre former for diabetes    0.024512\n",
      "3                                    comp_neuro    0.023932\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "importances = best_xgb.feature_importances_\n",
    "fi = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(fi.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
