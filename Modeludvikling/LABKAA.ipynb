{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "LABKA_PATH = Path(r\"C:\\Users\\kfq6\\Documents\\Data\\LABKA.xlsx\")\n",
    "OUT_XLSX   = Path(r\"C:\\Users\\kfq6\\Documents\\Data\\LABKA_wide_numeric.xlsx\")\n",
    "OUT_PARQ   = Path(r\"C:\\Users\\kfq6\\Documents\\Data\\LABKA_wide_numeric.parquet\")\n",
    "\n",
    "ID_COL   = \"DW_EK_Borger\"\n",
    "DATE_SRC = \"Dato_Proevetagningstid\"   # source datetime col\n",
    "DATE_COL = \"Testdato\"                 # derived date col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def decode_excel_xml(s):\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return re.sub(r\"_x([0-9A-Fa-f]{4})_\", lambda m: chr(int(m.group(1), 16)), s)\n",
    "\n",
    "def make_col_name(analy, unit):\n",
    "    analy = \"\" if pd.isna(analy) else str(analy).strip()\n",
    "    unit  = \"\" if pd.isna(unit)  else str(unit).strip()\n",
    "    return f\"{analy} [{unit}]\" if unit else analy\n",
    "\n",
    "def coerce_numeric_series(s: pd.Series):\n",
    "    x = s.astype(str).str.strip()\n",
    "    # Decode Excel escapes\n",
    "    x = x.str.replace(r\"_x([0-9A-Fa-f]{4})_\", lambda m: chr(int(m.group(1),16)), regex=True)\n",
    "    # Censor flags\n",
    "    censor = pd.Series(0, index=x.index, dtype=\"int8\")\n",
    "    censor = censor.mask(x.str.contains(r\"^\\s*[<≤]\\s*\"), -1)\n",
    "    censor = censor.mask(x.str.contains(r\"^\\s*[>≥]\\s*\"), +1)\n",
    "    # Strip qualifiers and parentheses negatives\n",
    "    x = x.str.replace(r\"^\\s*[<≤>≥=]\\s*\", \"\", regex=True)\n",
    "    x = x.str.replace(r\"^\\((.*)\\)$\", r\"-\\1\", regex=True)\n",
    "    # Map Pos/Neg\n",
    "    lower = x.str.lower()\n",
    "    x = x.mask(lower.isin({\"pos\",\"positiv\",\"positive\"}), \"1\")\n",
    "    x = x.mask(lower.isin({\"neg\",\"negativ\",\"negative\"}), \"0\")\n",
    "    # Remove thousand dots only when followed by 3 digits\n",
    "    x = x.str.replace(r\"\\.(?=\\d{3}(\\D|$))\", \"\", regex=True)\n",
    "    # BUG FIX: must use .str.replace here, not Series.replace\n",
    "    x = x.str.replace(\",\", \".\", regex=False)\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return num, censor\n",
    "\n",
    "def sanitize_header(c: str) -> str:\n",
    "    c = str(c).replace(\"\\n\",\" \").replace(\"/\", \"_\").replace(\"  \",\" \").strip()\n",
    "    return c.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load once, as strings; decode\n",
    "# -----------------------------\n",
    "df = pd.read_excel(LABKA_PATH, engine=\"openpyxl\", dtype=str)\n",
    "df.columns = [decode_excel_xml(c).strip() for c in df.columns]\n",
    "df = df.applymap(decode_excel_xml)\n",
    "\n",
    "# Required cols\n",
    "cols = [\n",
    "    \"DW_EK_Borger\",\"Dato_Proevetagningstid\",\"Klok_Proevetagningstid\",\n",
    "    \"Alder_Proevetagningstid\",\"Analysenavn\",\"Svar\",\"Enhed\",\n",
    "]\n",
    "missing = [c for c in cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Mangler forventede kolonner: {missing}\")\n",
    "df = df[cols].copy()\n",
    "\n",
    "# Parse datetime (Danish), derive date\n",
    "df[DATE_SRC] = pd.to_datetime(df[DATE_SRC], errors=\"coerce\", dayfirst=True)\n",
    "df[DATE_COL] = df[DATE_SRC].dt.date\n",
    "\n",
    "# Coerce numeric ONCE on long data\n",
    "df[\"Svar_num\"], df[\"Svar_censor\"] = coerce_numeric_series(df[\"Svar\"])\n",
    "\n",
    "# Build key; try to backfill missing unit with most common unit per analyte\n",
    "df[\"Analysenavn_clean\"] = df[\"Analysenavn\"].astype(str).str.strip()\n",
    "unit_mode = (df.dropna(subset=[\"Enhed\"])\n",
    "               .groupby(\"Analysenavn_clean\")[\"Enhed\"]\n",
    "               .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else None))\n",
    "df[\"Enhed_filled\"] = df.apply(\n",
    "    lambda r: r[\"Enhed\"] if pd.notna(r[\"Enhed\"]) and str(r[\"Enhed\"]).strip() != \"\" \n",
    "              else unit_mode.get(r[\"Analysenavn_clean\"], None),\n",
    "    axis=1\n",
    ")\n",
    "df[\"AnalyseKolonne\"] = [make_col_name(a, u) for a, u in zip(df[\"Analysenavn_clean\"], df[\"Enhed_filled\"])]\n",
    "\n",
    "# Deduplicate: last result per patient-day-test\n",
    "df_last = (\n",
    "    df.sort_values([ID_COL, DATE_COL, \"AnalyseKolonne\", DATE_SRC])\n",
    "      .groupby([ID_COL, DATE_COL, \"AnalyseKolonne\"], as_index=False)\n",
    "      .agg(\n",
    "          Svar_num=(\"Svar_num\",\"mean\"),   # mean if multiple within day after dedup safety\n",
    "          n_meas=(\"Svar_num\",\"count\"),\n",
    "          Alder_Proevetagningstid=(\"Alder_Proevetagningstid\",\"first\"),\n",
    "          Dato_Proevetagningstid=(DATE_SRC,\"last\"),\n",
    "          Klok_Proevetagningstid=(\"Klok_Proevetagningstid\",\"last\"),\n",
    "      )\n",
    ")\n",
    "\n",
    "# Pivot numeric wide\n",
    "wide = df_last.pivot(\n",
    "    index=[ID_COL, DATE_COL, \"Alder_Proevetagningstid\",\"Dato_Proevetagningstid\",\"Klok_Proevetagningstid\"],\n",
    "    columns=\"AnalyseKolonne\",\n",
    "    values=\"Svar_num\"\n",
    ").reset_index()\n",
    "\n",
    "# Sanitize headers\n",
    "wide.columns = [sanitize_header(c) for c in wide.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric lab columns: 17\n",
      "Sample lab columns: ['B-Hæmoglobin_[mmol_l]', 'Hb(B)-Hæmoglobin_A1c_(IFCC)_[mmol_mol]', 'P-25-Hydroxy-Vitamin_D(D3+D2)_[nmol_l]', 'P-Albumin_[g_l]', 'P-Calcium_(albuminkorrigeret)_[mmol_l]', 'P-Calcium_[mmol_l]', 'P-Kalium_[mmol_l]', 'P-Kolesterol_HDL_[mmol_l]', 'P-Kolesterol_LDL_[mmol_l]', 'P-Kolesterol_[mmol_l]']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Collapse duplicate pairs: no-unit vs unit\n",
    "# Keep unit version if both exist; drop empty duplicates\n",
    "# -----------------------------\n",
    "meta_keep = {\n",
    "    ID_COL, DATE_COL, \"Alder_Proevetagningstid\",\"Dato_Proevetagningstid\",\"Klok_Proevetagningstid\"\n",
    "}\n",
    "\n",
    "cols_set = set(wide.columns)\n",
    "unit_cols = [c for c in wide.columns if \"_[\" in c and c not in meta_keep]\n",
    "drop_cols = []\n",
    "\n",
    "# Map unitless -> unit candidates\n",
    "# e.g. \"P-Kalium\" vs \"P-Kalium_[mmol_l]\"\n",
    "base_names = {}\n",
    "for c in unit_cols:\n",
    "    base = c.split(\"_[\", 1)[0]  # strip unit suffix\n",
    "    base_names.setdefault(base, []).append(c)\n",
    "\n",
    "for base, with_units in base_names.items():\n",
    "    if base in cols_set:\n",
    "        # If unitless exists and any unit column exists, prefer unit column(s)\n",
    "        # If unitless is fully empty or duplicates existing non-null values, drop it\n",
    "        u = base\n",
    "        # consider it empty if all NaN\n",
    "        if wide[u].isna().all():\n",
    "            drop_cols.append(u)\n",
    "        else:\n",
    "            # if exactly one unit column, and unitless has no extra info, collapse: fillna unitless into unit col, drop unitless\n",
    "            if len(with_units) == 1:\n",
    "                wu = with_units[0]\n",
    "                # if unit col has many nulls but unitless has values, move them across\n",
    "                wide[wu] = wide[wu].fillna(wide[u])\n",
    "                drop_cols.append(u)\n",
    "            else:\n",
    "                # multiple units: keep them; drop unitless if it adds nothing\n",
    "                if wide[u].isna().all():\n",
    "                    drop_cols.append(u)\n",
    "\n",
    "# Drop fully empty analyte columns (all NaN)\n",
    "empty_cols = [c for c in wide.columns if c not in meta_keep and wide[c].isna().all()]\n",
    "drop_cols = sorted(set(drop_cols + empty_cols))\n",
    "if drop_cols:\n",
    "    wide = wide.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide table saved to: C:\\Users\\kfq6\\Documents\\Data\\LABKA_wide_rawSvar.xlsx\n",
      "Shape: (57480, 22)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Save result\n",
    "# -----------------------------\n",
    "num_df.to_excel(OUT_PATH, index=False)\n",
    "print(\"Wide table saved to:\", OUT_PATH)\n",
    "print(\"Shape:\", num_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
